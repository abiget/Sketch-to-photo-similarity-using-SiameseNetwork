{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "\n",
    "%matplotlib inline\n",
    "import torchvision\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.utils\n",
    "import numpy as np\n",
    "import random\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import PIL.ImageOps    \n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import zipfile\n",
    "import torchvision.models as models\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mount Driver\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Methods to display image for visualization\n",
    "\n",
    "def imshow(img,text=None,should_save=False):\n",
    "    npimg = img.numpy()\n",
    "    plt.axis(\"off\")\n",
    "    if text:\n",
    "        plt.text(75, 8, text, style='italic',fontweight='bold',\n",
    "            bbox={'facecolor':'white', 'alpha':0.8, 'pad':10})\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()    \n",
    "\n",
    "def show_plot(iteration,loss):\n",
    "    plt.plot(iteration,loss)\n",
    "    plt.title('Number of Epochs vs Loss ')\n",
    "    plt.xlabel('Number of Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Contrastive loss\n",
    "\n",
    "class ContrastiveLoss(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Contrastive loss function.\n",
    "    Based on: http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, margin=2.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, output1, output2, label):\n",
    "        euclidean_distance = F.pairwise_distance(output1, output2)\n",
    "        loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +\n",
    "                                      (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))\n",
    "\n",
    "        return loss_contrastive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SpatialPyramid_pooling\n",
    "\n",
    "import math\n",
    "def spatial_pyramid_pool(previous_conv, num_sample, previous_conv_size, out_pool_size_row=[5, 4, 3, 2, 1, 3, 1], out_pool_size_height=[5, 4, 3, 2, 1, 2, 3]):\n",
    "    '''\n",
    "    previous_conv: a tensor vector of previous convolution layer\n",
    "    num_sample: an int number of image in the batch\n",
    "    previous_conv_size: an int vector [height, width] of the matrix features size of previous convolution layer\n",
    "    out_pool_size: a int vector of expected output size of max pooling layer\n",
    "    \n",
    "    returns: a tensor vector with shape [1 x n] is the concentration of multi-level pooling\n",
    "    '''    \n",
    "    # print(previous_conv.size())\n",
    "    for i in range(len(out_pool_size_row)):\n",
    "        # print(previous_conv_size)\n",
    "        h_wid = int(math.ceil(previous_conv_size[0] / out_pool_size_height[i]))\n",
    "        w_wid = int(math.ceil(previous_conv_size[1] / out_pool_size_row[i]))\n",
    "        h_pad = (h_wid*out_pool_size_height[i] - previous_conv_size[0] + 1)/2\n",
    "        w_pad = (w_wid*out_pool_size_row[i] - previous_conv_size[1] + 1)/2\n",
    "        maxpool = nn.MaxPool2d((h_wid, w_wid), stride=(h_wid, w_wid), padding=(int(math.floor(h_pad)), int(math.floor(w_pad))))\n",
    "        x = maxpool(previous_conv)\n",
    "        # print(x.size())\n",
    "        if(i == 0):\n",
    "            # print(\"\\X size:\",x.size())\n",
    "            spp = x.view(x.size()[0], x.size()[1],-1, 1)\n",
    "            # print(\"\\nspp size:\",spp.size())\n",
    "        else:\n",
    "            spp = torch.cat((spp,x.view(x.size()[0], x.size()[1],-1, 1)), 2)\n",
    "            # print(\"\\nsize:\",spp.size())\n",
    "    # print(\"\\nsize:\",spp.size())\n",
    "     \n",
    "    return spp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SiameseNetwork Architecture\n",
    "# # spatial_out_size = [  #spatial pyramid out put for 7 level-pyramid\n",
    "# #  [5,5], \n",
    "# #  [4,4], \n",
    "# #  [3, 3], \n",
    "# #  [2, 2], \n",
    "# #  [1, 1],\n",
    "# #  [3, 2],\n",
    "# #  [1, 3]\n",
    "# # ]\n",
    "# out_spatial_size_row =    [5, 4, 3, 2, 1]\n",
    "# out_spatial_size_height = [5, 4, 3, 2, 1]\n",
    "\n",
    "class CNN1(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    # self.output_num_row = out_spatial_size_row\n",
    "    # self.output_num_height = out_spatial_size_height\n",
    "\n",
    "    self.pad = nn.ZeroPad2d(1)\n",
    "    self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1)#, padding=1\n",
    "    self.conv2 = nn.Conv2d(64, 64, kernel_size=3, stride=1)#, padding=1\n",
    "    self.max_pool1 = nn.MaxPool2d(kernel_size=2, stride=2) #, padding=0\n",
    "\n",
    "    self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1)#, padding=1\n",
    "    self.conv4 = nn.Conv2d(128, 128, kernel_size=3, stride=1)#, padding=1\n",
    "    self.conv5 = nn.Conv2d(128, 256, kernel_size=3, stride=1)#, padding=1\n",
    "    self.conv6 = nn.Conv2d(256, 256, kernel_size=3, stride=1)#, padding=1\n",
    "\n",
    "    self.conv7 = nn.Conv2d(256, 256, kernel_size=3, stride=1)#, padding=1\n",
    "    self.conv8 = nn.Conv2d(256, 512, kernel_size=3, stride=1)#, padding=1\n",
    "    self.conv9 = nn.Conv2d(512, 512, kernel_size=3, stride=1)#, padding=1\n",
    "    self.conv10 = nn.Conv2d(512, 512, kernel_size=3, stride=1)#, padding=1\n",
    "\n",
    "    self.conv11 = nn.Conv2d(512, 512, kernel_size=3, stride=1)#, padding=1\n",
    "    self.conv12 = nn.Conv2d(512, 512, kernel_size=3, stride=1)#, padding=1\n",
    "    self.conv13 = nn.Conv2d(512, 512, kernel_size=3, stride=1)#, padding=1\n",
    "    #upto here the output is of size = (512, 14, 14)\n",
    "\n",
    "    #spatial dimesionality reduction \n",
    "    self.conv14 = nn.Conv2d(512, 64, kernel_size=3, padding=1, stride=1)\n",
    "    self.conv15 = nn.Conv2d(64, 64, kernel_size=1, stride=1) #something here\n",
    "    self.fc1 = nn.Sequential(\n",
    "        nn.Linear(64*64*1, 500),\n",
    "        nn.ReLU(inplace=True),\n",
    "\n",
    "        nn.Linear(500, 500),\n",
    "        nn.ReLU(inplace=True),\n",
    "\n",
    "        nn.Linear(500, 50)\n",
    "    )\n",
    "  def forward_once(self, x):\n",
    "      output = self.pad(F.relu(self.conv1(self.pad(x)))) #conv1\n",
    "      output = self.pad(self.max_pool1(F.relu(self.conv2(output)))) #conv2\n",
    "      output = self.pad(F.relu(self.conv3(output))) #conv3\n",
    "      output = self.pad(self.max_pool1(F.relu(self.conv4(output)))) #conv4\n",
    "      \n",
    "      output = self.pad(F.relu(self.conv5(output)))#conv5\n",
    "      output = self.pad(F.relu(self.conv6(output))) #conv 6\n",
    "      output = self.pad(self.max_pool1(F.relu(self.conv7(output)))) #conv7\n",
    "      output = self.pad(F.relu(self.conv8(output))) #conv 8\n",
    "      output = self.pad(F.relu(self.conv9(output))) #conv 9\n",
    "      output = self.pad(self.max_pool1(F.relu(self.conv10(output)))) #conv10\n",
    "      output = self.pad(F.relu(self.conv11(output))) #conv 11\n",
    "      output = self.pad(F.relu(self.conv12(output))) #conv 12\n",
    "      output = F.relu(self.conv13(output)) #conv13\n",
    "      \n",
    "      \n",
    "\n",
    "      output = spatial_pyramid_pool(output, 1, [int(output.size(2)), int(output.size(3))])\n",
    "      \n",
    "      #spatial dimesionality reduction\n",
    "      output = F.relu(self.conv14(output)) #conv14\n",
    "      output = F.relu(self.conv15(output)) #conv15\n",
    "\n",
    "      #size upto here (-1, 64, 64, 1)\n",
    "\n",
    "      output = output.view(output.size()[0], 1, output.size()[1], output.size()[2])\n",
    "      #size upto here (-1, 1, 64, 64)\n",
    "      \n",
    "      output = output.view(output.size()[0], -1)\n",
    "      #to check how it works\n",
    "      output = self.fc1(output)\n",
    "      return output\n",
    "\n",
    "  #return a size of (2*64*64, 1-d) for each image\n",
    "  def forward(self, input1, input2): #add here ''\n",
    "      output1 = self.forward_once(input1)\n",
    "      # ssp1 = spatial_pyramid_pool(output1, 1, [int(output1.size(2)), int(output1.size(3))], self.out_spatial_size)\n",
    "      output2 = self.forward_once(input2)\n",
    "      # ssp2 = spatial_pyramid_pool(output2, 1, [int(output2.size(2)), int(output2.size(3))], self.out_spatial_size)\n",
    "      return output1, output2 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and Load Models\n",
    "# to save and load the model\n",
    "def save_models(cnn1_model, name):\n",
    "  # torch.save(cnn1_model, \"SavedModel/\"+name+\".pt\")\n",
    "  torch.save(cnn1_model, \"/content/drive/MyDrive/SavedModel/FinalModelSaved/\"+name+\".pt\")\n",
    "  \n",
    "def load_models( name):\n",
    "  # cnn1_model=torch.load(\"SavedModel/\"+name+\".pt\")\n",
    "  cnn1_model=torch.load(\"/content/drive/MyDrive/SavedModel/FinalModelSaved/\"+name+\".pt\")\n",
    "  return cnn1_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load Keras Weight into PyTorch model\n",
    "\n",
    "# pip install deepface\n",
    "\n",
    "#calling the dependencies\n",
    "from deepface import DeepFace\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "  #calling VGGFace\n",
    "model_name = \"VGG-Face\"\n",
    "model = DeepFace.build_model(model_name)\n",
    "\n",
    "weights = model.get_weights() #load parameters from vggface\n",
    "\n",
    "# Create model CNN1\n",
    "cnn1_model = CNN1().cuda()\n",
    "cnn1_model\n",
    "\n",
    "# from torchsummary import summary\n",
    "# summary(cnn1_model, (3, 224, 224), (3, 224, 224))\n",
    "\n",
    "# Create model CNN2\n",
    "# cnn2_model = CNN2().cuda()\n",
    "# cnn2_model\n",
    "\n",
    "# from torchsummary import summary\n",
    "# summary(cnn2_model, (2, 64, 64))\n",
    "\n",
    "def allocate_keras_vggface_weight_to_pyt():\n",
    "  conv_layers = [cnn1_model.conv1, cnn1_model.conv2, cnn1_model.conv3, cnn1_model.conv4, \n",
    "               cnn1_model.conv5, cnn1_model.conv6, cnn1_model.conv7, cnn1_model.conv8, \n",
    "               cnn1_model.conv9, cnn1_model.conv10, cnn1_model.conv11, cnn1_model.conv12, \n",
    "               cnn1_model.conv13]\n",
    "  index_count = 0\n",
    "  for conv in conv_layers:\n",
    "    conv.weight.data = torch.from_numpy(np.transpose(weights[index_count])).cuda()\n",
    "    index_count += 1\n",
    "    conv.bias.data = torch.from_numpy(weights[index_count]).cuda()\n",
    "    index_count += 1\n",
    "allocate_keras_vggface_weight_to_pyt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Visualization\n",
    "\n",
    "#set path to the dataset \n",
    "class Config():\n",
    "    training_dir = \"/content/drive/MyDrive/FinalDataSet/AllDatasets/Training\"\n",
    "    testing_dir = \"/content/drive/MyDrive/FinalDataSet/AllDatasets/Testing\"\n",
    "    # training_dir = \"/content/drive/MyDrive/training\"\n",
    "    # testing_dir = \"/content/drive/MyDrive/testing\"\n",
    "    train_batch_size = 32\n",
    "    train_number_epochs = 400\n",
    "\n",
    "Config.training_dir\n",
    "\n",
    "#custom dataset \n",
    "class SiameseNetworkDataset(Dataset):\n",
    "    \n",
    "    def __init__(self,imageFolderDataset,transform=None,should_invert=False):\n",
    "        self.imageFolderDataset = imageFolderDataset    \n",
    "        self.transform = transform\n",
    "        self.should_invert = should_invert\n",
    "        \n",
    "    def __getitem__(self,index):\n",
    "        img0_tuple = random.choice(self.imageFolderDataset.imgs)\n",
    "        #we need to make sure approx 50% of images are in the same class\n",
    "        should_get_same_class = random.randint(0,1) \n",
    "        if should_get_same_class:\n",
    "            while True:\n",
    "                #keep looping till the same class image is found\n",
    "                img1_tuple = random.choice(self.imageFolderDataset.imgs) \n",
    "                if img0_tuple[1]==img1_tuple[1]:\n",
    "                    break\n",
    "        else:\n",
    "            while True:\n",
    "                #keep looping till a different class image is found\n",
    "                \n",
    "                img1_tuple = random.choice(self.imageFolderDataset.imgs) \n",
    "                if img0_tuple[1] !=img1_tuple[1]:\n",
    "                    break\n",
    "\n",
    "        img0 = Image.open(img0_tuple[0])\n",
    "        img1 = Image.open(img1_tuple[0])\n",
    "        img0 = img0.convert(\"RGB\")\n",
    "        img1 = img1.convert(\"RGB\")\n",
    "        \n",
    "        if self.should_invert:\n",
    "            img0 = PIL.ImageOps.invert(img0)\n",
    "            img1 = PIL.ImageOps.invert(img1)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img0 = self.transform(img0)\n",
    "            img1 = self.transform(img1)\n",
    "        \n",
    "        return img0, img1 , torch.from_numpy(np.array([int(img1_tuple[1]!=img0_tuple[1])],dtype=np.float32))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.imageFolderDataset.imgs)\n",
    "\n",
    "folder_dataset = dset.ImageFolder(root=Config.training_dir)\n",
    "\n",
    "siamese_dataset = SiameseNetworkDataset(imageFolderDataset=folder_dataset,\n",
    "                                        transform=transforms.Compose([transforms.Resize((224,224)),\n",
    "                                                                      transforms.ToTensor()\n",
    "                                                                      ])\n",
    "                                       ,should_invert=False)\n",
    "\n",
    "visual_dataloader = DataLoader(siamese_dataset,\n",
    "                        shuffle=True,\n",
    "                        num_workers=2,\n",
    "                        batch_size=32)\n",
    "dataiter = iter(visual_dataloader)\n",
    "\n",
    "\n",
    "example_batch = next(dataiter)\n",
    "concatenated = torch.cat((example_batch[0],example_batch[1]),0)\n",
    "imshow(torchvision.utils.make_grid(concatenated))\n",
    "print(example_batch[2].numpy())\n",
    "\n",
    "\n",
    "example_batch[1].size(), example_batch[0].size() # [batch_size, #channels, width, height]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
