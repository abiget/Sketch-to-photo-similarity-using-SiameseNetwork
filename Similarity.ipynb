{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary packages\n",
    "\n",
    "%matplotlib inline\n",
    "import torchvision\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.utils\n",
    "import numpy as np\n",
    "import random\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import PIL.ImageOps    \n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import zipfile\n",
    "import torchvision.models as models\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mount Driver\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Methods to display image for visualization\n",
    "\n",
    "def imshow(img,text=None,should_save=False):\n",
    "    npimg = img.numpy()\n",
    "    plt.axis(\"off\")\n",
    "    if text:\n",
    "        plt.text(75, 8, text, style='italic',fontweight='bold',\n",
    "            bbox={'facecolor':'white', 'alpha':0.8, 'pad':10})\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()    \n",
    "\n",
    "def show_plot(iteration,loss):\n",
    "    plt.plot(iteration,loss)\n",
    "    plt.title('Number of Epochs vs Loss ')\n",
    "    plt.xlabel('Number of Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Contrastive loss\n",
    "\n",
    "class ContrastiveLoss(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Contrastive loss function.\n",
    "    Based on: http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, margin=2.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, output1, output2, label):\n",
    "        euclidean_distance = F.pairwise_distance(output1, output2)\n",
    "        loss_contrastive = torch.mean((1-label) * torch.pow(euclidean_distance, 2) +\n",
    "                                      (label) * torch.pow(torch.clamp(self.margin - euclidean_distance, min=0.0), 2))\n",
    "\n",
    "        return loss_contrastive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SpatialPyramid_pooling\n",
    "\n",
    "import math\n",
    "def spatial_pyramid_pool(previous_conv, num_sample, previous_conv_size, out_pool_size_row=[5, 4, 3, 2, 1, 3, 1], out_pool_size_height=[5, 4, 3, 2, 1, 2, 3]):\n",
    "    '''\n",
    "    previous_conv: a tensor vector of previous convolution layer\n",
    "    num_sample: an int number of image in the batch\n",
    "    previous_conv_size: an int vector [height, width] of the matrix features size of previous convolution layer\n",
    "    out_pool_size: a int vector of expected output size of max pooling layer\n",
    "    \n",
    "    returns: a tensor vector with shape [1 x n] is the concentration of multi-level pooling\n",
    "    '''    \n",
    "    # print(previous_conv.size())\n",
    "    for i in range(len(out_pool_size_row)):\n",
    "        # print(previous_conv_size)\n",
    "        h_wid = int(math.ceil(previous_conv_size[0] / out_pool_size_height[i]))\n",
    "        w_wid = int(math.ceil(previous_conv_size[1] / out_pool_size_row[i]))\n",
    "        h_pad = (h_wid*out_pool_size_height[i] - previous_conv_size[0] + 1)/2\n",
    "        w_pad = (w_wid*out_pool_size_row[i] - previous_conv_size[1] + 1)/2\n",
    "        maxpool = nn.MaxPool2d((h_wid, w_wid), stride=(h_wid, w_wid), padding=(int(math.floor(h_pad)), int(math.floor(w_pad))))\n",
    "        x = maxpool(previous_conv)\n",
    "        # print(x.size())\n",
    "        if(i == 0):\n",
    "            # print(\"\\X size:\",x.size())\n",
    "            spp = x.view(x.size()[0], x.size()[1],-1, 1)\n",
    "            # print(\"\\nspp size:\",spp.size())\n",
    "        else:\n",
    "            spp = torch.cat((spp,x.view(x.size()[0], x.size()[1],-1, 1)), 2)\n",
    "            # print(\"\\nsize:\",spp.size())\n",
    "    # print(\"\\nsize:\",spp.size())\n",
    "     \n",
    "    return spp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SiameseNetwork Architecture\n",
    "# # spatial_out_size = [  #spatial pyramid out put for 7 level-pyramid\n",
    "# #  [5,5], \n",
    "# #  [4,4], \n",
    "# #  [3, 3], \n",
    "# #  [2, 2], \n",
    "# #  [1, 1],\n",
    "# #  [3, 2],\n",
    "# #  [1, 3]\n",
    "# # ]\n",
    "# out_spatial_size_row =    [5, 4, 3, 2, 1]\n",
    "# out_spatial_size_height = [5, 4, 3, 2, 1]\n",
    "\n",
    "class CNN1(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    # self.output_num_row = out_spatial_size_row\n",
    "    # self.output_num_height = out_spatial_size_height\n",
    "\n",
    "    self.pad = nn.ZeroPad2d(1)\n",
    "    self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1)#, padding=1\n",
    "    self.conv2 = nn.Conv2d(64, 64, kernel_size=3, stride=1)#, padding=1\n",
    "    self.max_pool1 = nn.MaxPool2d(kernel_size=2, stride=2) #, padding=0\n",
    "\n",
    "    self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1)#, padding=1\n",
    "    self.conv4 = nn.Conv2d(128, 128, kernel_size=3, stride=1)#, padding=1\n",
    "    self.conv5 = nn.Conv2d(128, 256, kernel_size=3, stride=1)#, padding=1\n",
    "    self.conv6 = nn.Conv2d(256, 256, kernel_size=3, stride=1)#, padding=1\n",
    "\n",
    "    self.conv7 = nn.Conv2d(256, 256, kernel_size=3, stride=1)#, padding=1\n",
    "    self.conv8 = nn.Conv2d(256, 512, kernel_size=3, stride=1)#, padding=1\n",
    "    self.conv9 = nn.Conv2d(512, 512, kernel_size=3, stride=1)#, padding=1\n",
    "    self.conv10 = nn.Conv2d(512, 512, kernel_size=3, stride=1)#, padding=1\n",
    "\n",
    "    self.conv11 = nn.Conv2d(512, 512, kernel_size=3, stride=1)#, padding=1\n",
    "    self.conv12 = nn.Conv2d(512, 512, kernel_size=3, stride=1)#, padding=1\n",
    "    self.conv13 = nn.Conv2d(512, 512, kernel_size=3, stride=1)#, padding=1\n",
    "    #upto here the output is of size = (512, 14, 14)\n",
    "\n",
    "    #spatial dimesionality reduction \n",
    "    self.conv14 = nn.Conv2d(512, 64, kernel_size=3, padding=1, stride=1)\n",
    "    self.conv15 = nn.Conv2d(64, 64, kernel_size=1, stride=1) #something here\n",
    "    self.fc1 = nn.Sequential(\n",
    "        nn.Linear(64*64*1, 500),\n",
    "        nn.ReLU(inplace=True),\n",
    "\n",
    "        nn.Linear(500, 500),\n",
    "        nn.ReLU(inplace=True),\n",
    "\n",
    "        nn.Linear(500, 50)\n",
    "    )\n",
    "  def forward_once(self, x):\n",
    "      output = self.pad(F.relu(self.conv1(self.pad(x)))) #conv1\n",
    "      output = self.pad(self.max_pool1(F.relu(self.conv2(output)))) #conv2\n",
    "      output = self.pad(F.relu(self.conv3(output))) #conv3\n",
    "      output = self.pad(self.max_pool1(F.relu(self.conv4(output)))) #conv4\n",
    "      \n",
    "      output = self.pad(F.relu(self.conv5(output)))#conv5\n",
    "      output = self.pad(F.relu(self.conv6(output))) #conv 6\n",
    "      output = self.pad(self.max_pool1(F.relu(self.conv7(output)))) #conv7\n",
    "      output = self.pad(F.relu(self.conv8(output))) #conv 8\n",
    "      output = self.pad(F.relu(self.conv9(output))) #conv 9\n",
    "      output = self.pad(self.max_pool1(F.relu(self.conv10(output)))) #conv10\n",
    "      output = self.pad(F.relu(self.conv11(output))) #conv 11\n",
    "      output = self.pad(F.relu(self.conv12(output))) #conv 12\n",
    "      output = F.relu(self.conv13(output)) #conv13\n",
    "      \n",
    "      \n",
    "\n",
    "      output = spatial_pyramid_pool(output, 1, [int(output.size(2)), int(output.size(3))])\n",
    "      \n",
    "      #spatial dimesionality reduction\n",
    "      output = F.relu(self.conv14(output)) #conv14\n",
    "      output = F.relu(self.conv15(output)) #conv15\n",
    "\n",
    "      #size upto here (-1, 64, 64, 1)\n",
    "\n",
    "      output = output.view(output.size()[0], 1, output.size()[1], output.size()[2])\n",
    "      #size upto here (-1, 1, 64, 64)\n",
    "      \n",
    "      output = output.view(output.size()[0], -1)\n",
    "      #to check how it works\n",
    "      output = self.fc1(output)\n",
    "      return output\n",
    "\n",
    "  #return a size of (2*64*64, 1-d) for each image\n",
    "  def forward(self, input1, input2): #add here ''\n",
    "      output1 = self.forward_once(input1)\n",
    "      # ssp1 = spatial_pyramid_pool(output1, 1, [int(output1.size(2)), int(output1.size(3))], self.out_spatial_size)\n",
    "      output2 = self.forward_once(input2)\n",
    "      # ssp2 = spatial_pyramid_pool(output2, 1, [int(output2.size(2)), int(output2.size(3))], self.out_spatial_size)\n",
    "      return output1, output2 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save and Load Models\n",
    "# to save and load the model\n",
    "def save_models(cnn1_model, name):\n",
    "  # torch.save(cnn1_model, \"SavedModel/\"+name+\".pt\")\n",
    "  torch.save(cnn1_model, \"/content/drive/MyDrive/SavedModel/FinalModelSaved/\"+name+\".pt\")\n",
    "  \n",
    "def load_models( name):\n",
    "  # cnn1_model=torch.load(\"SavedModel/\"+name+\".pt\")\n",
    "  cnn1_model=torch.load(\"/content/drive/MyDrive/SavedModel/FinalModelSaved/\"+name+\".pt\")\n",
    "  return cnn1_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load Keras Weight into PyTorch model\n",
    "\n",
    "# pip install deepface\n",
    "\n",
    "#calling the dependencies\n",
    "from deepface import DeepFace\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "  #calling VGGFace\n",
    "model_name = \"VGG-Face\"\n",
    "model = DeepFace.build_model(model_name)\n",
    "\n",
    "weights = model.get_weights() #load parameters from vggface\n",
    "\n",
    "# Create model CNN1\n",
    "cnn1_model = CNN1().cuda()\n",
    "cnn1_model\n",
    "\n",
    "# from torchsummary import summary\n",
    "# summary(cnn1_model, (3, 224, 224), (3, 224, 224))\n",
    "\n",
    "# Create model CNN2\n",
    "# cnn2_model = CNN2().cuda()\n",
    "# cnn2_model\n",
    "\n",
    "# from torchsummary import summary\n",
    "# summary(cnn2_model, (2, 64, 64))\n",
    "\n",
    "def allocate_keras_vggface_weight_to_pyt():\n",
    "  conv_layers = [cnn1_model.conv1, cnn1_model.conv2, cnn1_model.conv3, cnn1_model.conv4, \n",
    "               cnn1_model.conv5, cnn1_model.conv6, cnn1_model.conv7, cnn1_model.conv8, \n",
    "               cnn1_model.conv9, cnn1_model.conv10, cnn1_model.conv11, cnn1_model.conv12, \n",
    "               cnn1_model.conv13]\n",
    "  index_count = 0\n",
    "  for conv in conv_layers:\n",
    "    conv.weight.data = torch.from_numpy(np.transpose(weights[index_count])).cuda()\n",
    "    index_count += 1\n",
    "    conv.bias.data = torch.from_numpy(weights[index_count]).cuda()\n",
    "    index_count += 1\n",
    "allocate_keras_vggface_weight_to_pyt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Visualization\n",
    "\n",
    "#set path to the dataset \n",
    "class Config():\n",
    "    training_dir = \"/content/drive/MyDrive/FinalDataSet/AllDatasets/Training\"\n",
    "    testing_dir = \"/content/drive/MyDrive/FinalDataSet/AllDatasets/Testing\"\n",
    "    # training_dir = \"/content/drive/MyDrive/training\"\n",
    "    # testing_dir = \"/content/drive/MyDrive/testing\"\n",
    "    train_batch_size = 32\n",
    "    train_number_epochs = 400\n",
    "\n",
    "Config.training_dir\n",
    "\n",
    "#custom dataset \n",
    "class SiameseNetworkDataset(Dataset):\n",
    "    \n",
    "    def __init__(self,imageFolderDataset,transform=None,should_invert=False):\n",
    "        self.imageFolderDataset = imageFolderDataset    \n",
    "        self.transform = transform\n",
    "        self.should_invert = should_invert\n",
    "        \n",
    "    def __getitem__(self,index):\n",
    "        img0_tuple = random.choice(self.imageFolderDataset.imgs)\n",
    "        #we need to make sure approx 50% of images are in the same class\n",
    "        should_get_same_class = random.randint(0,1) \n",
    "        if should_get_same_class:\n",
    "            while True:\n",
    "                #keep looping till the same class image is found\n",
    "                img1_tuple = random.choice(self.imageFolderDataset.imgs) \n",
    "                if img0_tuple[1]==img1_tuple[1]:\n",
    "                    break\n",
    "        else:\n",
    "            while True:\n",
    "                #keep looping till a different class image is found\n",
    "                \n",
    "                img1_tuple = random.choice(self.imageFolderDataset.imgs) \n",
    "                if img0_tuple[1] !=img1_tuple[1]:\n",
    "                    break\n",
    "\n",
    "        img0 = Image.open(img0_tuple[0])\n",
    "        img1 = Image.open(img1_tuple[0])\n",
    "        img0 = img0.convert(\"RGB\")\n",
    "        img1 = img1.convert(\"RGB\")\n",
    "        \n",
    "        if self.should_invert:\n",
    "            img0 = PIL.ImageOps.invert(img0)\n",
    "            img1 = PIL.ImageOps.invert(img1)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img0 = self.transform(img0)\n",
    "            img1 = self.transform(img1)\n",
    "        \n",
    "        return img0, img1 , torch.from_numpy(np.array([int(img1_tuple[1]!=img0_tuple[1])],dtype=np.float32))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.imageFolderDataset.imgs)\n",
    "\n",
    "folder_dataset = dset.ImageFolder(root=Config.training_dir)\n",
    "\n",
    "siamese_dataset = SiameseNetworkDataset(imageFolderDataset=folder_dataset,\n",
    "                                        transform=transforms.Compose([transforms.Resize((224,224)),\n",
    "                                                                      transforms.ToTensor()\n",
    "                                                                      ])\n",
    "                                       ,should_invert=False)\n",
    "\n",
    "visual_dataloader = DataLoader(siamese_dataset,\n",
    "                        shuffle=True,\n",
    "                        num_workers=2,\n",
    "                        batch_size=32)\n",
    "dataiter = iter(visual_dataloader)\n",
    "\n",
    "\n",
    "example_batch = next(dataiter)\n",
    "concatenated = torch.cat((example_batch[0],example_batch[1]),0)\n",
    "imshow(torchvision.utils.make_grid(concatenated))\n",
    "print(example_batch[2].numpy())\n",
    "\n",
    "\n",
    "example_batch[1].size(), example_batch[0].size() # [batch_size, #channels, width, height]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#**Training !**\n",
    "\n",
    "train_dataloader = DataLoader(siamese_dataset,\n",
    "                        shuffle=True,\n",
    "                        num_workers=2,\n",
    "                        batch_size=Config.train_batch_size)\n",
    "\n",
    "criterion = ContrastiveLoss()\n",
    "optimizer = optim.Adam(cnn1_model.parameters(),lr = 0.00005 )\n",
    "\n",
    "counter = []\n",
    "loss_history = [] \n",
    "iteration_number= 0\n",
    "\n",
    "#Load the saved model\n",
    "cnn1_model_name = 'cnn1_model_name1'\n",
    "cnn1_model = load_models(cnn1_model_name)\n",
    "loss = 0.0\n",
    "\n",
    "for epoch in range(0,Config.train_number_epochs):\n",
    "    for i, data in enumerate(train_dataloader,0):\n",
    "        img0, img1 , label = data\n",
    "        img0, img1 , label = Variable(img0).cuda(), Variable(img1).cuda() , Variable(label).cuda()\n",
    "        # output1, output2 = cnn2_model(cnn1_model_output1, cnn1_model_output2)\n",
    "        cnn1_model_output1, cnn1_model_output2 = cnn1_model(img0, img1)\n",
    "        optimizer.zero_grad()\n",
    "        loss_contrastive = criterion(cnn1_model_output1,cnn1_model_output2,label)\n",
    "        loss_contrastive.backward()\n",
    "        optimizer.step()\n",
    "        loss += loss_contrastive.item()\n",
    "        if i %10 == 0 and i > 1:\n",
    "            iteration_number +=10\n",
    "            counter.append(iteration_number)\n",
    "            loss_history.append(loss/10)\n",
    "            fin = open(\"/content/drive/MyDrive/SavedModel/data2.txt\", \"a\")\n",
    "            fin.write('\\n'+str(loss/10))\n",
    "            fin.close()\n",
    "            loss = 0.0\n",
    "    print(\"Epoch number {}\\n \\n\".format(epoch))\n",
    "    \n",
    "    #save the model every 50 epochs\n",
    "    if epoch %  2 == 0 and epoch > 0:\n",
    "      save_models(cnn1_model, 'cnn1_model_name1')\n",
    "    \n",
    "show_plot(counter,loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testing !\n",
    "\n",
    "#Load and Visualization Testing datasets  \n",
    "\n",
    "\n",
    "#Testing and Training folders\n",
    "testing_folder_dataset = dset.ImageFolder(root=Config.testing_dir) \n",
    "training_folder_dataset = dset.ImageFolder(root=Config.training_dir)\n",
    "\n",
    "#Training and Testing datasets\n",
    "testing_siamese_dataset = SiameseNetworkDataset(imageFolderDataset=testing_folder_dataset,\n",
    "                                        transform=transforms.Compose([transforms.Resize((224,224)),\n",
    "                                                                      transforms.ToTensor()\n",
    "                                                                      ])\n",
    "                                       ,should_invert=False)\n",
    "training_siamese_dataset = SiameseNetworkDataset(imageFolderDataset=training_folder_dataset,\n",
    "                                        transform=transforms.Compose([transforms.Resize((224,224)),\n",
    "                                                                      transforms.ToTensor()\n",
    "                                                                      ])\n",
    "                                       ,should_invert=False)\n",
    "\n",
    "#Training datset visualizations\n",
    "vis_train_dataloader = DataLoader(training_siamese_dataset,\n",
    "                        shuffle=False,\n",
    "                        num_workers=8,\n",
    "                        batch_size=12)\n",
    "dataiter = iter(vis_train_dataloader)\n",
    "x01,x1,a= next(dataiter)\n",
    "concatenated = torch.cat((x01,x1),0)\n",
    "imshow(torchvision.utils.make_grid(concatenated))\n",
    "print(a)\n",
    "\n",
    "#Testing datset visualizations \n",
    "vis_test_dataloader = DataLoader(testing_siamese_dataset,\n",
    "                        shuffle=False,\n",
    "                        num_workers=8,\n",
    "                        batch_size=12)\n",
    "dataiter = iter(vis_test_dataloader)\n",
    "\n",
    "x01,x1,a= next(dataiter)\n",
    "concatenated = torch.cat((x01,x1),0)\n",
    "imshow(torchvision.utils.make_grid(concatenated))\n",
    "print(a,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing with training data \n",
    "\n",
    "#Load the saved model for testing\n",
    "cnn1_model_name = 'cnn1_model_name1'\n",
    "cnn1_model = load_models(cnn1_model_name)\n",
    "\n",
    "cnn1_model\n",
    "\n",
    "#test with training data\n",
    "train_dataloader = DataLoader(training_siamese_dataset,num_workers=2,batch_size=1,shuffle=False)\n",
    "dataiter = iter(train_dataloader)\n",
    "\n",
    "#Threshold for the euclidain distance is 0.75\n",
    "threshold_dis = [8]\n",
    "correct_match_prediction_label = 0\n",
    "correct_count = 0\n",
    "total_count = 1440 #examples for testing from trainset\n",
    "# for thre in threshold_dis:\n",
    "thre = 8\n",
    "for i in range(total_count):\n",
    "    x0,x1,actual_label = next(dataiter) #img0&img1\n",
    "    output1,output2 = cnn1_model(Variable(x0).cuda(),Variable(x1).cuda())\n",
    "    euclidean_distance =F.pairwise_distance(output1, output2)\n",
    "    # print(\"Dis: \",euclidean_distance, \"\\n\")\n",
    "    if euclidean_distance <= thre:\n",
    "      if actual_label == correct_match_prediction_label:\n",
    "        correct_count += 1\n",
    "accuracy = (correct_count / total_count) * 100\n",
    "print(r\"Training Accuracy : {}, with threshold : {}\".format(accuracy, thre))\n",
    "# print(r\"Avarage distance : {}, with threshold : {}\".format(total_distance, threshold_dis))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonestration with test data\n",
    "\n",
    "\n",
    "# from numpy.random import randint\n",
    "from torchvision import datasets, transforms\n",
    "import os\n",
    "# from pathlib import Path\n",
    "# from os import path\n",
    "# from torchvision.utils import save_image\n",
    "# # import torch\n",
    "# # import torchvision\n",
    "transform2 = transforms.Compose([transforms.ToTensor(),transforms.Resize((224,224))])\n",
    "# import shutil, sys \n",
    "# import torch\n",
    "\n",
    "# import matplotlib as plt\n",
    "# import matplotlib.pylab as plt\n",
    "# import numpy as np\n",
    "# import torchvision.utils as vutils\n",
    "\n",
    "\n",
    "\n",
    "# !unzip /content/drive/MyDrive/FinalDataSet/bedilu.zip /content/drive/MyDrive/FinalDataSet/\n",
    "\n",
    "\n",
    "# parent = '/content/drive/MyDrive/FinalDataSet/AllDatasets/Testing'\n",
    "anchor_dir = '/content/drive/MyDrive/FinalDataSet/sketch'\n",
    "comp_dir = '/content/drive/MyDrive/FinalDataSet/photo/'\n",
    "\n",
    "dataset1 = datasets.ImageFolder(anchor_dir, transform=transform2) #sketch\n",
    "anchor = torch.utils.data.DataLoader(dataset1, batch_size=1,shuffle=True)\n",
    "\n",
    "dataset2 = datasets.ImageFolder(comp_dir, transform=transform2) #photo\n",
    "comparing = torch.utils.data.DataLoader(dataset2, batch_size=1,shuffle=True)\n",
    "\n",
    "# dataset = datasets.ImageFolder(anchor_dir, transform=transform2)\n",
    "# anchor = torch.utils.data.DataLoader(dataset, batch_size=1,shuffle=True)\n",
    "# # length = len(os.listdir(comp_dir))\n",
    "# #we can change comparing batch size to increase the compared images for each sketch\n",
    "# dataset2 = datasets.ImageFolder(comp_dir, transform=transform2)\n",
    "# comparing = torch.utils.data.DataLoader(dataset2, batch_size=10,shuffle=True)\n",
    "# for i in range(total_count):\n",
    "#       x0 ,x1,actual_label = next(dataiter) #img0&img1\n",
    "#       output1,output2 = cnn1_model(Variable(x0).cuda(),Variable(x1).cuda())\n",
    "#       euclidean_distance =F.pairwise_distance(output1, output2)\n",
    "#       if euclidean_distance <= thre:\n",
    "#         if actual_label == correct_match_prediction_label:\n",
    "#           correct_count += 1\n",
    "#   accuracy = (correct_count / total_count) * 100\n",
    "#   print(r\"Testing Accuracy : {}, with threshold : {}\".format(accuracy, thre))\n",
    "\n",
    "\n",
    "# threshold_dis = [9]\n",
    "correct_match_prediction_label = 0\n",
    "total_count = 20 #examples for testing from trainset\n",
    "thre = 1.2\n",
    "\n",
    "\n",
    "\n",
    "correct_count = 0\n",
    "anch,label1 = next(iter(anchor))\n",
    "euclidean_distance = []\n",
    "concatenated = []\n",
    "# here we can implement for loop to get another shuffled comaring images\n",
    "for item in range(total_count):\n",
    "    comp,label2 = next(iter(comparing))\n",
    "    concatenated.append(torch.cat((anch,comp),0))\n",
    "    output1, output2 = cnn1_model(Variable(anch).cuda(),Variable(comp).cuda())\n",
    "    #here we can find each image in the testing dataloader, to compare with the anchor\n",
    "    # euclidean_distance = F.pairwise_distance(output1, output2)\n",
    "    euclidean_distance.append(F.pairwise_distance(output1, output2).item())\n",
    "\n",
    "    if euclidean_distance[item] <= thre:\n",
    "      if label1 == label2:\n",
    "        correct_count += 1\n",
    "    else:\n",
    "      if label1 != label2:\n",
    "        correct_count+= 1\n",
    "        # print(correct_count)\n",
    "    # print(label1,label2,euclidean_distance)\n",
    "    imshow(torchvision.utils.make_grid(concatenated[item]),'Dissimilarity: {:.2f}'.format(euclidean_distance[item]))\n",
    "\n",
    "accuracy = (correct_count/total_count)*100\n",
    "# print(r\"Testing Accuracy : {}, with threshold : {}, correct count{}\".format(accuracy, thre, correct_count))\n",
    "euclidean_distance_series = pd.Series(euclidean_distance)\n",
    "small_dis_index = euclidean_distance_series.argmin()\n",
    "print(\"The most matching images are :\\n\")\n",
    "imshow(torchvision.utils.make_grid(concatenated[small_dis_index]),'Dissimilarity: {:.2f}'.format(euclidean_distance[small_dis_index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
